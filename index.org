







* Leaf

We use Leaf as a data structure because I'm not insane.
#+BEGIN_SRC python :tangle DTLearner.py
class Leaf(object):

    def __init__(self,label):
        if label is None:
            raise ValueError      # If no value is passed, then this leaf node is horrible.
        self.label = label
#+END_SRC


* Node

#+BEGIN_SRC python :tangle DTLearner.py
class Node(object):

    def __init__(self,value,left,right):
        if value is None:
            raise ValueError      # If no value is not given, this node is invalid.

        self.value = value
        self.left  = left
        self.right = right
#+END_SRC


* RTLearner components

** Init

 The init function only takes in values that describe the tree independently of its training values. This includes values like leaf size and if the tree is verbose or not. 
 


- What is X & Y and why do I care about it?

So your decision tree is effectively a function. It takes in n arguments and spits out one value as a result. The size of the X data is the size of arguments that your function takes in and the Y data is always one column long. 


- What is leaf_size? Why do I care about it.
So decision trees aren't perfect and don't need to be perfect. Sometimes you want a decision tree that's very rough, othertimes rather fine. 
Leaf size is analogous to the size of a hole in a sieve - the smaller you make the leaf_size, the more precise you make your decision tree.
If the leaf size is 1, the decision is the finest possible and it cannot get finer. If it is more than 1 (say n), 
it breaks down the data you give it until you get a chunk of data that's very close to each other (close enough to basically be the same thing) and happens to be of size n. 
You take the result of all those n values and take the mean - the mean represents the "leaf."

Thus you can think of it as taking your data, sorting it and finding the mean of every n values. It might sound like a dumb idea but it makes sense for very large datasets.


- Why don't we pass training data in the init function? This is dumb.
Actually adding data is done in the addEvidence function. The reason behind this is that you might want to add training data to an existing decision tree. 
When this happens, you add the new data and rebuild the decision tree. You might ask "Why not just take all the data you already have and make a new decision tree?" 
Because it makes more sense to not recreate new datasets in memory.

 #+NAME: init
 #+BEGIN_SRC python :noweb yes
def __init__(self, leaf_size):

    if leaf_size is None:
       leaf_size = 1
    self.dataX = None                 # Training data is initially nothing
    self.dataY = None                 # Training data is initially nothing
    self.leaf_size = leaf_size
 #+END_SRC


** addEvidence
#+NAME: addEvidence
#+BEGIN_SRC python :noweb yes
def addEvidence(self,dataX,dataY):
     # Take the old data you already have and add the new data to that.
    self.dataX = mergeData(self.dataX, dataX)
    self.dataY = mergeData(self.dataY, dataY)

     # The old decision tree is no longer valid. make a new tree.
    self.random_tree = self.build_tree(self.dataX, self.dataY)

    
    def mergeData(oldData, newData):
        # Append new data to old data.
        return np.append(oldData, newData) if oldData else newData
        # TODO then prune out values that are not unique. np.unique doesn't seem to work as expected...
        #return np.unique(np.append(oldData, newData) if oldData else newData)
#+END_SRC

** buildTree
#+NAME: buildTree
#+BEGIN_SRC python :noweb yes
def buildTree(self, dataX, dataY):
    self.tree = build_tree_helper(dataX,dataY)


def build_tree_helper(self, dataX, dataY):
    # TODO check shape of both X andy Y?
    # Return leaf if:
    # 1. There is only one element left.
    # 2. There is less than or equal to the given leaf size elements.
    # 3. All remaining Y values are the same.
    if dataX.shape[0] == 1 or dataX.shape[0] <= self.leaf_size or all(dataY[0] == y for y in dataY):
        yMean = dataY.mean()
        return Leaf(ymean)
       # Determine random feature i to split on.

    i =  np.random.randint(dataX.shape[1])
     # Select 2 random values from ith feature.
    a = np.random.randint(dataX.shape[0])
    b = np.random.randint(dataX.shape[0])


    # Take avergage of the 2 randomly selected values.
    split_value = (dataX[a,i] + dataX[b,i]) / 2

    # Build left tree such that we pass it data that's less than or equal to the split value.
    # li is a set of boolean values that are true if the condition below is true. Think like Matlab
    li = dataX[:,i] < split_value
    left_tree = self.build_tree_helper(dataX[li,:], dataY[li])
    # Build left tree such that we pass it data that's greater than the split value.
    ri = dataX[:,i] > split_value
    right_tree = self.build_tree_helper(dataX[ri,:], dataY[ri])
    
     return Node(value = split_value,
                left = left_tree,
                right = right_tree)
#+END_SRC




** Query

questionValue is a pandas data frame.
#+NAME: query
#+BEGIN_SRC python :noweb yes
def query(self,questions):

    # Get a value for each point given.
    results = []

    for i in range(0, questions.shape[0]):
        results.append(self.queryHelper(question[i]))

    # Convert and return.
    return np.array(results)



def queryHelper(self,node,questionValue):

    if isinstance(node,Leaf):
      return node.label

    if questionValue > node.value:
        queryHelper(node.right,questionValue)
    else: queryHelper(node.left,questionValue)
#+END_SRC











* RTLearner implementation
#+BEGIN_SRC python :tangle DTLearner.py
class RTLearner(object):

  <<init>>
  <<addEvidence>>
  <<buildTree>>
  <<query>>
#+END_SRC




